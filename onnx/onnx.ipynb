{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ec211ae-e2ae-4fa9-834f-7553a43cdf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import onnx \n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "import torch_tensorrt\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.utils import benchmark\n",
    "\n",
    "import torchvision.models as models\n",
    "import transformers\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from thop import profile as thop_profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24ba0863-bc23-49f6-9f1b-84c0b4befa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:1'\n",
    "\n",
    "# Init Input Data\n",
    "inputs = torch.randint(0, 1000, (1, 128))\n",
    "pixels = torch.randn((1,3,224,224))\n",
    "\n",
    "# Init Models\n",
    "bert = transformers.BertModel.from_pretrained('bert-base-uncased', torchscript=True)\n",
    "funnel = transformers.FunnelModel.from_pretrained('funnel-transformer/small', torchscript=True)\n",
    "\n",
    "resnet18 = models.resnet18()\n",
    "resnet50 = models.resnet50()\n",
    "wide_resnet = models.wide_resnet50_2()\n",
    "maskrcnn = models.detection.maskrcnn_resnet50_fpn()\n",
    "resnet152 = models.resnet152()\n",
    "alexnet = models.alexnet()\n",
    "\n",
    "# Move data to GPU\n",
    "bert = bert.to(DEVICE).eval()\n",
    "funnel = funnel.to(DEVICE).eval()\n",
    "\n",
    "resnet18 = resnet18.to(DEVICE).eval()\n",
    "resnet50 = resnet50.to(DEVICE).eval()\n",
    "resnet152 = resnet152.to(DEVICE).eval()\n",
    "wide_resnet = wide_resnet.to(DEVICE).eval()\n",
    "maskrcnn = maskrcnn.to(DEVICE).eval()\n",
    "alexnet = alexnet.to(DEVICE).eval()\n",
    "\n",
    "pixels = pixels.to(DEVICE)\n",
    "inputs = inputs.to(DEVICE)\n",
    "\n",
    "\n",
    "# Warm up Runs\n",
    "_ = bert(inputs)\n",
    "_ = funnel(inputs)\n",
    "\n",
    "_ = resnet18(pixels)\n",
    "_ = resnet50(pixels)\n",
    "_ = wide_resnet(pixels)\n",
    "_ = resnet152(pixels)\n",
    "_ = alexnet(pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5d415566-04f0-49a2-9b1a-f8c8d7d17a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_resnet = torch.jit.script(resnet, pixels)\n",
    "jit_maskrcnn = torch.jit.script(maskrcnn, pixels)\n",
    "\n",
    "# jit_bert = torch.jit.trace(bert, inputs)\n",
    "# jit_funnel = torch.jit.trace(funnel, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "747517d6-e98e-4fa9-bc3f-bdf5e3a9d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnasnet_fp16 = models.mnasnet1_0().eval().half().to(DEVICE)\n",
    "torch.onnx.export(mnasnet_fp16, pixels, 'mnasnet-fp16-dynamic.onnx', \n",
    "                  export_params=True, opset_version=10, do_constant_folding=True,\n",
    "                  input_names=['pixels'], output_names=['output'],\n",
    "                  dynamic_axes={\"pixels\": {0: \"batch\"}, \"output\": {0: \"batch\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4fe44072-a8aa-4309-b5f4-b56a5a60d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "resnet18_fp16 = resnet18.half()\n",
    "pixels = torch.randn((1,3,224,224), dtype=torch.float16).to('cuda:1')\n",
    "\n",
    "torch.onnx.export(resnet18_fp16, pixels, 'resnet18-fp16-dynamic.onnx', \n",
    "                  export_params=True, opset_version=10, do_constant_folding=True,\n",
    "                  input_names=['inputs'], output_names=['output'],\n",
    "                  dynamic_axes={\"inputs\": {0: \"batch\"}, \"outputs\": {0: \"batch\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a637b935-6ea9-496e-bc3a-c4cf825028d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.9/site-packages/torch/nn/functional.py:3896: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  (torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i], dtype=torch.float32)).float()))\n",
      "/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.9/site-packages/torchvision/models/detection/anchor_utils.py:121: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.empty((), dtype=torch.int64, device=device).fill_(image_size[0] // g[0]),\n",
      "/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.9/site-packages/torchvision/models/detection/anchor_utils.py:122: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.empty((), dtype=torch.int64, device=device).fill_(image_size[1] // g[1]),\n",
      "/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.9/site-packages/torchvision/models/detection/rpn.py:99: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  A = Ax4 // 4\n",
      "/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.9/site-packages/torchvision/models/detection/rpn.py:100: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  C = AxC // A\n",
      "/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.9/site-packages/torchvision/ops/boxes.py:157: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n",
      "/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.9/site-packages/torchvision/ops/boxes.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))\n",
      "/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.9/site-packages/torch/__init__.py:833: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n",
      "/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.9/site-packages/torchvision/models/detection/transform.py:298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
      "/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.9/site-packages/torchvision/models/detection/transform.py:299: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  / torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
      "/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.9/site-packages/torchvision/models/detection/roi_heads.py:389: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(M + 2 * padding).to(torch.float32) / torch.tensor(M).to(torch.float32)\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:4189: UserWarning: Exporting aten::index operator of advanced indexing in opset 13 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.9/site-packages/transformers/models/funnel/modeling_funnel.py:323: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  num_remove = shift * len(pooled_pos)\n",
      "/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.9/site-packages/transformers/models/funnel/modeling_funnel.py:669: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  pooling_flag = pooling_flag and block_index > 0\n",
      "/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.9/site-packages/transformers/models/funnel/modeling_funnel.py:498: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  shift = 2 if q_head.shape[1] != context_len else 1\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(resnet18, pixels, 'resnet18-dynamic.onnx', \n",
    "                  export_params=True, opset_version=10, do_constant_folding=True,\n",
    "                  input_names=['pixels'], output_names=['output'],\n",
    "                  dynamic_axes={\"pixels\": {0: \"batch\"}, \"output\": {0: \"batch\"}})\n",
    "\n",
    "torch.onnx.export(resnet, pixels, 'resnet50-dynamic.onnx', \n",
    "                  export_params=True, opset_version=10, do_constant_folding=True,\n",
    "                  input_names=['pixels'], output_names=['output'],\n",
    "                  dynamic_axes={\"pixels\": {0: \"batch\"}, \"output\": {0: \"batch\"}})\n",
    "\n",
    "\n",
    "torch.onnx.export(resnet152, pixels, 'resnet152-dynamic.onnx', \n",
    "                  export_params=True, opset_version=10, do_constant_folding=True,\n",
    "                  input_names=['pixels'], output_names=['output'],\n",
    "                  dynamic_axes={\"pixels\": {0: \"batch\"}, \"output\": {0: \"batch\"}})\n",
    "\n",
    "torch.onnx.export(wide_resnet, pixels, 'wideresnet-dynamic.onnx', \n",
    "                  export_params=True, opset_version=10, do_constant_folding=True,\n",
    "                  input_names=['pixels'], output_names=['output'],\n",
    "                  dynamic_axes={\"pixels\": {0: \"batch\"}, \"output\": {0: \"batch\"}})\n",
    "\n",
    "torch.onnx.export(maskrcnn, pixels, 'maskrcnn-dynamic.onnx', \n",
    "                  export_params=True, opset_version=13, do_constant_folding=True,\n",
    "                  input_names=['pixels'], output_names=['output'],\n",
    "                  dynamic_axes={\"pixels\": {0: \"batch\"}, \"output\": {0: \"batch\"}})\n",
    "\n",
    "torch.onnx.export(bert, inputs, 'bert-dynamic.onnx',\n",
    "                  export_params=True, opset_version=13, do_constant_folding=True,\n",
    "                  input_names=['ids'], output_names=['output'],\n",
    "                  dynamic_axes={\"ids\": {0: \"batch\"}, \"output\": {0: \"batch\"}})\n",
    "\n",
    "torch.onnx.export(funnel, inputs, 'funnel-dynamic.onnx',\n",
    "                  export_params=True, opset_version=13, do_constant_folding=True,\n",
    "                  input_names=['ids'], output_names=['output'],\n",
    "                  dynamic_axes={\"ids\": {0: \"batch\"}, \"output\": {0: \"batch\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1232fc1-3104-41e5-81d5-6b7d0dd19bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = [\n",
    "    # ('CUDAExecutionProvider', {\n",
    "    #     'device_id': 1,\n",
    "    #     'arena_extend_strategy': 'kNextPowerOfTwo',\n",
    "    #     'gpu_mem_limit': 2 * 1024 * 1024 * 1024,\n",
    "    #     'cudnn_conv_algo_search': 'EXHAUSTIVE',\n",
    "    #     'do_copy_in_default_stream': True,\n",
    "    # }),\n",
    "    'CPUExecutionProvider',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7935a0-c13c-4837-904c-f93b966d7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_bind_rgb(io_binding, bs, dtype, use_cuda=False):\n",
    "    pixels_cpu = torch.randn((bs,3,224,224), dtype=dtype).cpu().numpy()\n",
    "    out = torch.empty((bs,1000), dtype=dtype).cpu().numpy()\n",
    "    device = 'cuda' if use_cuda else 'cpu'\n",
    "    pixels_ortvalue = ort.OrtValue.ortvalue_from_numpy(pixels_cpu, device, 0)\n",
    "    out_ortvalue = ort.OrtValue.ortvalue_from_numpy(out, device, 0)\n",
    "    io_binding.bind_ortvalue_input('pixels', pixels_ortvalue)\n",
    "    io_binding.bind_ortvalue_output('output', out_ortvalue)\n",
    "    return pixels_ortvalue, out_ortvalue \n",
    "\n",
    "def create_and_bind_ids(io_binding, bs,):\n",
    "    ids = torch.randint(0, 1000, (1, 128)).cpu().numpy()\n",
    "    ids_ortvalue = ort.OrtValue.ortvalue_from_numpy(ids, 'cuda', 0)\n",
    "    io_binding.bind_ortvalue_input('ids', ids_ortvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f5c7cc2-794d-4864-b4d0-b4263d64c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_model(onnx_fpath, providers, batch_size, dtype):\n",
    "    session = ort.InferenceSession(onnx_fpath, providers=providers)\n",
    "    io_binding = session.io_binding()\n",
    "\n",
    "    # One regular run for the necessary memory allocation and cuda graph capturing\n",
    "    create_and_bind_rgb(io_binding, batch_size, dtype)\n",
    "    session.run_with_iobinding(io_binding)\n",
    "\n",
    "    # One regular run for the necessary memory allocation and cuda graph capturing\n",
    "    create_and_bind_rgb(io_binding, batch_size, dtype)\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "                 on_trace_ready=torch.profiler.tensorboard_trace_handler(f'pytorch/log/{onnx_fpath}-batch{batch_size}'),\n",
    "                 record_shapes=True, with_flops=True, profile_memory=True) as prof:\n",
    "        with record_function('graph_inference'):\n",
    "            session.run_with_iobinding(io_binding)\n",
    "            \n",
    "def eval_latency_and_macs(model, onnx_fpath, providers=[\"CUDAExecutionProvider\"],dtype=torch.float32):\n",
    "    latency, macs = [], []\n",
    "    for bs in [1,2,3,4,6,8,16,32,64]:\n",
    "        print(f\"Evaluating {onnx_fpath} with batch size {bs}\")\n",
    "        session = ort.InferenceSession(onnx_fpath, providers=providers)\n",
    "        io_binding = session.io_binding()\n",
    "        create_and_bind_rgb(io_binding, bs,dtype)\n",
    "\n",
    "        # Warmup Run\n",
    "        session.run_with_iobinding(io_binding)\n",
    "        \n",
    "        # Measure ONNX latency\n",
    "        timer = benchmark.Timer(\n",
    "            stmt=\"session.run_with_iobinding(io_binding)\",\n",
    "            globals={\"session\": session, \"io_binding\": io_binding}\n",
    "        )\n",
    "        \n",
    "#         # Measure FLOPs\n",
    "#         inputs = torch.randn((bs,3,224,224), dtype=dtype).to('cuda:1')\n",
    "        latency.append(timer.timeit(100).mean)\n",
    "#         macs.append(thop_profile(model, (inputs,))[0])\n",
    "    return latency, macs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "203271ef-50de-440c-a7e7-b444dff720f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0034613483771681786\n",
      "0.004808780588209629\n",
      "0.006889117360115052\n",
      "0.008522093817591668\n",
      "0.011561025455594063\n",
      "0.014963545054197312\n",
      "0.02186247918754816\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "device = 'cpu'\n",
    "\n",
    "for bs in [1,2,3,4,6,8,12]:\n",
    "    session = ort.InferenceSession('resnet18-dynamic.onnx', providers=providers)\n",
    "    io_binding = session.io_binding()\n",
    "\n",
    "    pixels_cpu = torch.randn((bs ,3,224,224), dtype=dtype).cpu().numpy()\n",
    "    out = torch.empty((bs,1000), dtype=dtype).cpu().numpy()\n",
    "\n",
    "    pixels_ortvalue = ort.OrtValue.ortvalue_from_numpy(pixels_cpu, device, 0)\n",
    "    out_ortvalue = ort.OrtValue.ortvalue_from_numpy(out, device, 0)\n",
    "\n",
    "    io_binding.bind_ortvalue_input('pixels', pixels_ortvalue)\n",
    "    io_binding.bind_ortvalue_output('output', out_ortvalue)\n",
    "\n",
    "    timer = benchmark.Timer(\n",
    "        stmt=\"session.run_with_iobinding(io_binding)\",\n",
    "        globals={\"session\": session, \"io_binding\": io_binding}\n",
    "    )\n",
    "    print(timer.timeit(100).mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d06a5f49-e206-4cae-8e36-c63f1f100c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003409965671598911\n"
     ]
    }
   ],
   "source": [
    "# Measure ONNX latency\n",
    "timer = benchmark.Timer(\n",
    "    stmt=\"session.run_with_iobinding(io_binding)\",\n",
    "    globals={\"session\": session, \"io_binding\": io_binding}\n",
    ")\n",
    "print(timer.timeit(100).mean)\n",
    "\n",
    "        # latency_resnet18_cpu, macs_resnet18_cpu = eval_latency_and_macs(resnet18.cpu(), \"resnet18-dynamic.onnx\", providers, dtype=torch.float32)\n",
    "# profile_model(\"resnet18-fp16-dynamic.onnx\", providers, 8, torch.half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042b8c67-9399-4239-b5aa-b9be9ed031ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_latency_and_macs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m latency_resnet18_fp16, macs_resnet18 \u001b[38;5;241m=\u001b[39m \u001b[43meval_latency_and_macs\u001b[49m(resnet18\u001b[38;5;241m.\u001b[39mhalf(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet18-fp16-dynamic.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m, providers, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m      2\u001b[0m latency_mnasnet_fp16, macs_mnasnet_fp16 \u001b[38;5;241m=\u001b[39m eval_latency_and_macs(mnasnet\u001b[38;5;241m.\u001b[39mhalf(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnasnet-fp16-dynamic.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m, providers, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_latency_and_macs' is not defined"
     ]
    }
   ],
   "source": [
    "latency_resnet18_fp16, macs_resnet18 = eval_latency_and_macs(resnet18.half(), \"resnet18-fp16-dynamic.onnx\", providers, dtype=torch.float16)\n",
    "latency_mnasnet_fp16, macs_mnasnet_fp16 = eval_latency_and_macs(mnasnet.half(), \"mnasnet-fp16-dynamic.onnx\", providers, dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e8b2b-f324-46e1-916b-c397bae16550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "latency_resnet18, macs_resnet18 = eval_latency_and_macs(resnet18, \"resnet18-dynamic.onnx\", providers)\n",
    "latency_resnet50, macs_resnet50 = eval_latency_and_macs(resnet, \"resnet50-dynamic.onnx\", providers)\n",
    "latency_resnet152, macs_resnet152 = eval_latency_and_macs(resnet152, \"resnet152-dynamic.onnx\", providers)\n",
    "latency_wideresnet50, macs_wideresnet50 = eval_latency_and_macs(wide_resnet, \"wideresnet-dynamic.onnx\", providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2cf8c022-699b-4cbb-9fec-f7427c888b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0013843486830592156,\n",
       "  0.0012388412654399871,\n",
       "  0.0012840042635798455,\n",
       "  0.0013786252960562705,\n",
       "  0.0014433470740914346,\n",
       "  0.0017063410207629204,\n",
       "  0.0030685196071863173,\n",
       "  0.00543682936578989,\n",
       "  0.013508461378514768],\n",
       " [0.0015029918029904365,\n",
       "  0.0014550980925559997,\n",
       "  0.0015874777734279633,\n",
       "  0.001561770886182785,\n",
       "  0.0017023448646068572,\n",
       "  0.0018440712988376617,\n",
       "  0.003331167474389076,\n",
       "  0.00543928399682045,\n",
       "  0.010700393170118332])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latency_resnet18_fp16, latency_mnasnet_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cd7e006-fa3c-4397-90a0-cdd4835d4232",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'macs_mnasnet_fp16' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mmacs_mnasnet_fp16\u001b[49m, latency_mnasnet_fp16, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMnasnet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(macs_resnet18, latency_resnet18, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet18\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(batch, latency_resnet18_fp16, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet18 fp 16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'macs_mnasnet_fp16' is not defined"
     ]
    }
   ],
   "source": [
    "batch = [1, 2, 3, 4, 6, 8, 16, 32, 64]\n",
    "plt.plot(macs_mnasnet_fp16, latency_mnasnet_fp16, label=\"Mnasnet\")\n",
    "\n",
    "plt.plot(macs_resnet18, latency_resnet18, label=\"ResNet18\")\n",
    "plt.plot(batch, latency_resnet18_fp16, label=\"ResNet18 fp 16\")\n",
    "plt.plot(macs_resnet50, latency_resnet50, label=\"ResNet50\")\n",
    "plt.plot(macs_resnet152, latency_resnet152, label=\"ResNet152\")\n",
    "plt.plot(macs_wideresnet50, latency_wideresnet50, label=\"Wide ResNet50\")\n",
    "\n",
    "plt.xlabel('MACs')\n",
    "plt.ylabel('Latency')\n",
    "plt.title('ONNX with CUDA Backend on RTX-8000')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(batch, latency_resnet18_fp16, label=\"ResNet18 fp 16\")\n",
    "plt.plot(batch, latency_resnet18, label=\"ResNet18\")\n",
    "plt.plot(batch, latency_resnet50, label=\"ResNet50\")\n",
    "plt.plot(batch, latency_resnet152, label=\"Resnet152\")\n",
    "plt.plot(batch, latency_wideresnet50, label=\"Wide ResNet50\")\n",
    "\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Latency')\n",
    "plt.title('ONNX with CUDA Backend on RTX-8000')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4e59746-6460-4e68-a49d-7f848daa889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0020879823341965675, 0.0019292379915714264, 0.0019974905997514726, 0.0019260652735829354, 0.0028885624557733534, 0.003465595655143261, 0.005266679674386978, 0.009869575463235379, 0.018852105885744094]\n"
     ]
    }
   ],
   "source": [
    "BATCH = [1, 2, 3, 4, 6, 8, 16, 32, 64]\n",
    "\n",
    "\n",
    "jit_latency = []\n",
    "for b in BATCH:\n",
    "# Measure TorchScript Latency\n",
    "    ins = torch.randn((b, 3,224,224)).to('cuda:1')\n",
    "    timer = benchmark.Timer(\n",
    "        stmt=\"model(ins)\",\n",
    "        globals={\"model\": jit_resnet18, \"ins\": ins}\n",
    "    )\n",
    "    jit_latency.append(timer.timeit(100).mean)\n",
    "\n",
    "print(jit_latency)\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "             on_trace_ready=torch.profiler.tensorboard_trace_handler(f'pytorch/log/resnet18-vanilla'),\n",
    "             record_shapes=True, with_flops=True, profile_memory=True) as prof:\n",
    "    with record_function('graph_inference'):\n",
    "        resnet18(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a758ceee-6b74-4926-875f-0183cdf90c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0015636616200208663,\n",
       " 0.0014715524017810822,\n",
       " 0.0016967617347836495,\n",
       " 0.0018857941031455994,\n",
       " 0.002570772022008896,\n",
       " 0.0030693775415420533,\n",
       " 0.004890320859849453,\n",
       " 0.010048705376684666,\n",
       " 0.018493369854986667]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latency_resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052c8316-e639-4761-a2a3-f8ddd86acb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 1\n",
    "\n",
    "ins = torch.randn((BATCH, 3,224,224)).to('cuda:1')\n",
    "outs = torch.empty((BATCH, 1000), dtype=torch.float32).to('cuda:1')\n",
    "\n",
    "def bind_io(io_binding, ins, outs):\n",
    "    pixels_ortvalue = ort.OrtValue.ortvalue_from_numpy(ins.cpu().numpy(), 'cuda', 0)\n",
    "    out_ortvalue = ort.OrtValue.ortvalue_from_numpy(outs.cpu().numpy(), 'cuda', 0)\n",
    "\n",
    "    io_binding.bind_ortvalue_input('pixels', pixels_ortvalue)\n",
    "    io_binding.bind_ortvalue_output('output', out_ortvalue)\n",
    "\n",
    "session = ort.InferenceSession(\"resnet-dynamic.onnx\", providers=providers)\n",
    "io_binding = session.io_binding()\n",
    "bind_io(io_binding, ins, outs)\n",
    "\n",
    "session.run_with_iobinding(io_binding)\n",
    "onnx_out = io_binding.copy_outputs_to_cpu()[0]\n",
    "\n",
    "vanilla_out = resnet(ins)\n",
    "\n",
    "np.array(onnx_out) - vanilla_out.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50fd8c37-73f9-4436-b2fc-50fffafc53d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 16 14:41:30 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.82.00    Driver Version: 470.82.00    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     Off  | 00000000:19:00.0 Off |                  Off |\n",
      "| 33%   42C    P8    33W / 260W |  11197MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     Off  | 00000000:1A:00.0 Off |                  Off |\n",
      "| 33%   49C    P8    34W / 260W |   4191MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Quadro RTX 8000     Off  | 00000000:67:00.0 Off |                  Off |\n",
      "| 35%   45C    P8    30W / 260W |      8MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Quadro RTX 8000     Off  | 00000000:68:00.0 Off |                  Off |\n",
      "| 33%   47C    P8    26W / 260W |     40MiB / 48598MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1467      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A   2951978      C   ...baue/anaconda3/bin/python     8981MiB |\n",
      "|    0   N/A  N/A   3290508      C   ...e_benchmarking/bin/python      541MiB |\n",
      "|    0   N/A  N/A   3707169      C   ...envs/checklist/bin/python     1667MiB |\n",
      "|    1   N/A  N/A      1467      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   3290508      C   ...e_benchmarking/bin/python     4183MiB |\n",
      "|    2   N/A  N/A      1467      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A      1467      G   /usr/lib/xorg/Xorg                 18MiB |\n",
      "|    3   N/A  N/A      1796      G   /usr/bin/gnome-shell               17MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1446398-5d68-4e1f-bec5-74c15929604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = torch.cuda.Stream()\n",
    "stream.wait_stream(torch.cuda.current_stream())\n",
    "\n",
    "with torch.cuda.stream(stream):\n",
    "    resnet(pixels)\n",
    "    \n",
    "torch.cuda.current_stream().wait_stream(stream)\n",
    "\n",
    "graph = torch.cuda.CUDAGraph()\n",
    "with torch.cuda.graph(graph):\n",
    "    alexnet(pixels)\n",
    "\n",
    "graph.replay()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
