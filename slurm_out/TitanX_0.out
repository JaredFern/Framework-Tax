Loading bert model
INFO:TitanX:Loading bert model
Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
INFO:TitanX:Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
Benchmarking bert -- Batch: 64; Input: 128
INFO:TitanX:Benchmarking bert -- Batch: 64; Input: 128
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
INFO:TitanX:Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
Benchmarking bert -- Batch: 128; Input: 128
INFO:TitanX:Benchmarking bert -- Batch: 128; Input: 128
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading distilbert model
INFO:TitanX:Loading distilbert model
Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
INFO:TitanX:Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
Benchmarking distilbert -- Batch: 64; Input: 128
INFO:TitanX:Benchmarking distilbert -- Batch: 64; Input: 128
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
INFO:TitanX:Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
Benchmarking distilbert -- Batch: 128; Input: 128
INFO:TitanX:Benchmarking distilbert -- Batch: 128; Input: 128
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading funnel_transformer model
INFO:TitanX:Loading funnel_transformer model
Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
INFO:TitanX:Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 64; Input: 128
INFO:TitanX:Benchmarking funnel_transformer -- Batch: 64; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
INFO:TitanX:Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 128; Input: 128
INFO:TitanX:Benchmarking funnel_transformer -- Batch: 128; Input: 128
Loading albert model
INFO:TitanX:Loading albert model
Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
INFO:TitanX:Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
Benchmarking albert -- Batch: 64; Input: 128
INFO:TitanX:Benchmarking albert -- Batch: 64; Input: 128
Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
INFO:TitanX:Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
Benchmarking albert -- Batch: 128; Input: 128
INFO:TitanX:Benchmarking albert -- Batch: 128; Input: 128
Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading longformer model
INFO:TitanX:Loading longformer model
Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
INFO:TitanX:Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
Benchmarking longformer -- Batch: 64; Input: 128
INFO:TitanX:Benchmarking longformer -- Batch: 64; Input: 128
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
INFO:TitanX:Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
Benchmarking longformer -- Batch: 128; Input: 128
INFO:TitanX:Benchmarking longformer -- Batch: 128; Input: 128
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading mobile_bert model
INFO:TitanX:Loading mobile_bert model
Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
INFO:TitanX:Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 64; Input: 128
INFO:TitanX:Benchmarking mobile_bert -- Batch: 64; Input: 128
Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing MobileBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MobileBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
INFO:TitanX:Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 128; Input: 128
INFO:TitanX:Benchmarking mobile_bert -- Batch: 128; Input: 128
Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.dense.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing MobileBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MobileBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading squeeze_bert model
INFO:TitanX:Loading squeeze_bert model
Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
INFO:TitanX:Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 64; Input: 128
INFO:TitanX:Benchmarking squeeze_bert -- Batch: 64; Input: 128
Some weights of the model checkpoint at squeezebert/squeezebert-uncased were not used when initializing SqueezeBertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing SqueezeBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SqueezeBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
INFO:TitanX:Run Parameters: Namespace(act_fn='relu', batch_size=[64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-fp32', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='TitanX', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=False, use_ipex=False, use_jit=False, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 128; Input: 128
INFO:TitanX:Benchmarking squeeze_bert -- Batch: 128; Input: 128
Some weights of the model checkpoint at squeezebert/squeezebert-uncased were not used when initializing SqueezeBertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing SqueezeBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SqueezeBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
