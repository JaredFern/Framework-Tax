Loading vit32 model
INFO:A6000:Loading vit32 model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vit32 -- Batch: 1; Input: 224
INFO:A6000:Benchmarking vit32 -- Batch: 1; Input: 224
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/timm/models/layers/patch_embed.py:33: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert H == self.img_size[0] and W == self.img_size[1], \
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/timm/models/vision_transformer.py:186: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vit32 -- Batch: 2; Input: 224
INFO:A6000:Benchmarking vit32 -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vit32 -- Batch: 4; Input: 224
INFO:A6000:Benchmarking vit32 -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vit32 -- Batch: 8; Input: 224
INFO:A6000:Benchmarking vit32 -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vit32 -- Batch: 16; Input: 224
INFO:A6000:Benchmarking vit32 -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vit32 -- Batch: 32; Input: 224
INFO:A6000:Benchmarking vit32 -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vit32 -- Batch: 64; Input: 224
INFO:A6000:Benchmarking vit32 -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vit32', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vit32 -- Batch: 128; Input: 224
INFO:A6000:Benchmarking vit32 -- Batch: 128; Input: 224
Loading efficientnet model
INFO:A6000:Loading efficientnet model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet -- Batch: 1; Input: 224
INFO:A6000:Benchmarking efficientnet -- Batch: 1; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet -- Batch: 2; Input: 224
INFO:A6000:Benchmarking efficientnet -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet -- Batch: 4; Input: 224
INFO:A6000:Benchmarking efficientnet -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet -- Batch: 8; Input: 224
INFO:A6000:Benchmarking efficientnet -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet -- Batch: 16; Input: 224
INFO:A6000:Benchmarking efficientnet -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet -- Batch: 32; Input: 224
INFO:A6000:Benchmarking efficientnet -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet -- Batch: 64; Input: 224
INFO:A6000:Benchmarking efficientnet -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet -- Batch: 128; Input: 224
INFO:A6000:Benchmarking efficientnet -- Batch: 128; Input: 224
Loading efficientnet_lite model
INFO:A6000:Loading efficientnet_lite model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet_lite -- Batch: 1; Input: 224
INFO:A6000:Benchmarking efficientnet_lite -- Batch: 1; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet_lite -- Batch: 2; Input: 224
INFO:A6000:Benchmarking efficientnet_lite -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet_lite -- Batch: 4; Input: 224
INFO:A6000:Benchmarking efficientnet_lite -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet_lite -- Batch: 8; Input: 224
INFO:A6000:Benchmarking efficientnet_lite -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet_lite -- Batch: 16; Input: 224
INFO:A6000:Benchmarking efficientnet_lite -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet_lite -- Batch: 32; Input: 224
INFO:A6000:Benchmarking efficientnet_lite -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet_lite -- Batch: 64; Input: 224
INFO:A6000:Benchmarking efficientnet_lite -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='efficientnet_lite', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking efficientnet_lite -- Batch: 128; Input: 224
INFO:A6000:Benchmarking efficientnet_lite -- Batch: 128; Input: 224
Loading gernet model
INFO:A6000:Loading gernet model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking gernet -- Batch: 1; Input: 224
INFO:A6000:Benchmarking gernet -- Batch: 1; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking gernet -- Batch: 2; Input: 224
INFO:A6000:Benchmarking gernet -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking gernet -- Batch: 4; Input: 224
INFO:A6000:Benchmarking gernet -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking gernet -- Batch: 8; Input: 224
INFO:A6000:Benchmarking gernet -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking gernet -- Batch: 16; Input: 224
INFO:A6000:Benchmarking gernet -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking gernet -- Batch: 32; Input: 224
INFO:A6000:Benchmarking gernet -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking gernet -- Batch: 64; Input: 224
INFO:A6000:Benchmarking gernet -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='gernet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking gernet -- Batch: 128; Input: 224
INFO:A6000:Benchmarking gernet -- Batch: 128; Input: 224
Loading resnet18 model
INFO:A6000:Loading resnet18 model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnet18 -- Batch: 1; Input: 224
INFO:A6000:Benchmarking resnet18 -- Batch: 1; Input: 224
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnet18 -- Batch: 2; Input: 224
INFO:A6000:Benchmarking resnet18 -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnet18 -- Batch: 4; Input: 224
INFO:A6000:Benchmarking resnet18 -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnet18 -- Batch: 8; Input: 224
INFO:A6000:Benchmarking resnet18 -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnet18 -- Batch: 16; Input: 224
INFO:A6000:Benchmarking resnet18 -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnet18 -- Batch: 32; Input: 224
INFO:A6000:Benchmarking resnet18 -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnet18 -- Batch: 64; Input: 224
INFO:A6000:Benchmarking resnet18 -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnet18', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnet18 -- Batch: 128; Input: 224
INFO:A6000:Benchmarking resnet18 -- Batch: 128; Input: 224
Loading alexnet model
INFO:A6000:Loading alexnet model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking alexnet -- Batch: 1; Input: 224
INFO:A6000:Benchmarking alexnet -- Batch: 1; Input: 224
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking alexnet -- Batch: 2; Input: 224
INFO:A6000:Benchmarking alexnet -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking alexnet -- Batch: 4; Input: 224
INFO:A6000:Benchmarking alexnet -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking alexnet -- Batch: 8; Input: 224
INFO:A6000:Benchmarking alexnet -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking alexnet -- Batch: 16; Input: 224
INFO:A6000:Benchmarking alexnet -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking alexnet -- Batch: 32; Input: 224
INFO:A6000:Benchmarking alexnet -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking alexnet -- Batch: 64; Input: 224
INFO:A6000:Benchmarking alexnet -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='alexnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking alexnet -- Batch: 128; Input: 224
INFO:A6000:Benchmarking alexnet -- Batch: 128; Input: 224
Loading squeezenet model
INFO:A6000:Loading squeezenet model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeezenet -- Batch: 1; Input: 224
INFO:A6000:Benchmarking squeezenet -- Batch: 1; Input: 224
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeezenet -- Batch: 2; Input: 224
INFO:A6000:Benchmarking squeezenet -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeezenet -- Batch: 4; Input: 224
INFO:A6000:Benchmarking squeezenet -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeezenet -- Batch: 8; Input: 224
INFO:A6000:Benchmarking squeezenet -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeezenet -- Batch: 16; Input: 224
INFO:A6000:Benchmarking squeezenet -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeezenet -- Batch: 32; Input: 224
INFO:A6000:Benchmarking squeezenet -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeezenet -- Batch: 64; Input: 224
INFO:A6000:Benchmarking squeezenet -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='squeezenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeezenet -- Batch: 128; Input: 224
INFO:A6000:Benchmarking squeezenet -- Batch: 128; Input: 224
Loading vgg16 model
INFO:A6000:Loading vgg16 model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vgg16 -- Batch: 1; Input: 224
INFO:A6000:Benchmarking vgg16 -- Batch: 1; Input: 224
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vgg16 -- Batch: 2; Input: 224
INFO:A6000:Benchmarking vgg16 -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vgg16 -- Batch: 4; Input: 224
INFO:A6000:Benchmarking vgg16 -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vgg16 -- Batch: 8; Input: 224
INFO:A6000:Benchmarking vgg16 -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vgg16 -- Batch: 16; Input: 224
INFO:A6000:Benchmarking vgg16 -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vgg16 -- Batch: 32; Input: 224
INFO:A6000:Benchmarking vgg16 -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vgg16 -- Batch: 64; Input: 224
INFO:A6000:Benchmarking vgg16 -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='vgg16', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking vgg16 -- Batch: 128; Input: 224
INFO:A6000:Benchmarking vgg16 -- Batch: 128; Input: 224
Loading densenet model
INFO:A6000:Loading densenet model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking densenet -- Batch: 1; Input: 224
INFO:A6000:Benchmarking densenet -- Batch: 1; Input: 224
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking densenet -- Batch: 2; Input: 224
INFO:A6000:Benchmarking densenet -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking densenet -- Batch: 4; Input: 224
INFO:A6000:Benchmarking densenet -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking densenet -- Batch: 8; Input: 224
INFO:A6000:Benchmarking densenet -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking densenet -- Batch: 16; Input: 224
INFO:A6000:Benchmarking densenet -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking densenet -- Batch: 32; Input: 224
INFO:A6000:Benchmarking densenet -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking densenet -- Batch: 64; Input: 224
INFO:A6000:Benchmarking densenet -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='densenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking densenet -- Batch: 128; Input: 224
INFO:A6000:Benchmarking densenet -- Batch: 128; Input: 224
Loading inception model
INFO:A6000:Loading inception model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking inception -- Batch: 1; Input: 224
INFO:A6000:Benchmarking inception -- Batch: 1; Input: 224
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.
  warnings.warn(
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking inception -- Batch: 2; Input: 224
INFO:A6000:Benchmarking inception -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking inception -- Batch: 4; Input: 224
INFO:A6000:Benchmarking inception -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking inception -- Batch: 8; Input: 224
INFO:A6000:Benchmarking inception -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking inception -- Batch: 16; Input: 224
INFO:A6000:Benchmarking inception -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking inception -- Batch: 32; Input: 224
INFO:A6000:Benchmarking inception -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking inception -- Batch: 64; Input: 224
INFO:A6000:Benchmarking inception -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='inception', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking inception -- Batch: 128; Input: 224
INFO:A6000:Benchmarking inception -- Batch: 128; Input: 224
Loading googlenet model
INFO:A6000:Loading googlenet model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking googlenet -- Batch: 1; Input: 224
INFO:A6000:Benchmarking googlenet -- Batch: 1; Input: 224
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/googlenet.py:47: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.
  warnings.warn(
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking googlenet -- Batch: 2; Input: 224
INFO:A6000:Benchmarking googlenet -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking googlenet -- Batch: 4; Input: 224
INFO:A6000:Benchmarking googlenet -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking googlenet -- Batch: 8; Input: 224
INFO:A6000:Benchmarking googlenet -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking googlenet -- Batch: 16; Input: 224
INFO:A6000:Benchmarking googlenet -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking googlenet -- Batch: 32; Input: 224
INFO:A6000:Benchmarking googlenet -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking googlenet -- Batch: 64; Input: 224
INFO:A6000:Benchmarking googlenet -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='googlenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking googlenet -- Batch: 128; Input: 224
INFO:A6000:Benchmarking googlenet -- Batch: 128; Input: 224
Loading shufflenet model
INFO:A6000:Loading shufflenet model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking shufflenet -- Batch: 1; Input: 224
INFO:A6000:Benchmarking shufflenet -- Batch: 1; Input: 224
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/shufflenetv2.py:30: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  channels_per_group = num_channels // groups
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking shufflenet -- Batch: 2; Input: 224
INFO:A6000:Benchmarking shufflenet -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking shufflenet -- Batch: 4; Input: 224
INFO:A6000:Benchmarking shufflenet -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking shufflenet -- Batch: 8; Input: 224
INFO:A6000:Benchmarking shufflenet -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking shufflenet -- Batch: 16; Input: 224
INFO:A6000:Benchmarking shufflenet -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking shufflenet -- Batch: 32; Input: 224
INFO:A6000:Benchmarking shufflenet -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking shufflenet -- Batch: 64; Input: 224
INFO:A6000:Benchmarking shufflenet -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='shufflenet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking shufflenet -- Batch: 128; Input: 224
INFO:A6000:Benchmarking shufflenet -- Batch: 128; Input: 224
Loading mobilenet_v2 model
INFO:A6000:Loading mobilenet_v2 model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobilenet_v2 -- Batch: 1; Input: 224
INFO:A6000:Benchmarking mobilenet_v2 -- Batch: 1; Input: 224
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobilenet_v2 -- Batch: 2; Input: 224
INFO:A6000:Benchmarking mobilenet_v2 -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobilenet_v2 -- Batch: 4; Input: 224
INFO:A6000:Benchmarking mobilenet_v2 -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobilenet_v2 -- Batch: 8; Input: 224
INFO:A6000:Benchmarking mobilenet_v2 -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobilenet_v2 -- Batch: 16; Input: 224
INFO:A6000:Benchmarking mobilenet_v2 -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobilenet_v2 -- Batch: 32; Input: 224
INFO:A6000:Benchmarking mobilenet_v2 -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobilenet_v2 -- Batch: 64; Input: 224
INFO:A6000:Benchmarking mobilenet_v2 -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mobilenet_v2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobilenet_v2 -- Batch: 128; Input: 224
INFO:A6000:Benchmarking mobilenet_v2 -- Batch: 128; Input: 224
Loading resnext50_32x4d model
INFO:A6000:Loading resnext50_32x4d model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnext50_32x4d -- Batch: 1; Input: 224
INFO:A6000:Benchmarking resnext50_32x4d -- Batch: 1; Input: 224
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnext50_32x4d -- Batch: 2; Input: 224
INFO:A6000:Benchmarking resnext50_32x4d -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnext50_32x4d -- Batch: 4; Input: 224
INFO:A6000:Benchmarking resnext50_32x4d -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnext50_32x4d -- Batch: 8; Input: 224
INFO:A6000:Benchmarking resnext50_32x4d -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnext50_32x4d -- Batch: 16; Input: 224
INFO:A6000:Benchmarking resnext50_32x4d -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnext50_32x4d -- Batch: 32; Input: 224
INFO:A6000:Benchmarking resnext50_32x4d -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnext50_32x4d -- Batch: 64; Input: 224
INFO:A6000:Benchmarking resnext50_32x4d -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='resnext50_32x4d', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking resnext50_32x4d -- Batch: 128; Input: 224
INFO:A6000:Benchmarking resnext50_32x4d -- Batch: 128; Input: 224
Loading wide_resnet50_2 model
INFO:A6000:Loading wide_resnet50_2 model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking wide_resnet50_2 -- Batch: 1; Input: 224
INFO:A6000:Benchmarking wide_resnet50_2 -- Batch: 1; Input: 224
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking wide_resnet50_2 -- Batch: 2; Input: 224
INFO:A6000:Benchmarking wide_resnet50_2 -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking wide_resnet50_2 -- Batch: 4; Input: 224
INFO:A6000:Benchmarking wide_resnet50_2 -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking wide_resnet50_2 -- Batch: 8; Input: 224
INFO:A6000:Benchmarking wide_resnet50_2 -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking wide_resnet50_2 -- Batch: 16; Input: 224
INFO:A6000:Benchmarking wide_resnet50_2 -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking wide_resnet50_2 -- Batch: 32; Input: 224
INFO:A6000:Benchmarking wide_resnet50_2 -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking wide_resnet50_2 -- Batch: 64; Input: 224
INFO:A6000:Benchmarking wide_resnet50_2 -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='wide_resnet50_2', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking wide_resnet50_2 -- Batch: 128; Input: 224
INFO:A6000:Benchmarking wide_resnet50_2 -- Batch: 128; Input: 224
Loading mnasnet model
INFO:A6000:Loading mnasnet model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mnasnet -- Batch: 1; Input: 224
INFO:A6000:Benchmarking mnasnet -- Batch: 1; Input: 224
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mnasnet -- Batch: 2; Input: 224
INFO:A6000:Benchmarking mnasnet -- Batch: 2; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mnasnet -- Batch: 4; Input: 224
INFO:A6000:Benchmarking mnasnet -- Batch: 4; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mnasnet -- Batch: 8; Input: 224
INFO:A6000:Benchmarking mnasnet -- Batch: 8; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mnasnet -- Batch: 16; Input: 224
INFO:A6000:Benchmarking mnasnet -- Batch: 16; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mnasnet -- Batch: 32; Input: 224
INFO:A6000:Benchmarking mnasnet -- Batch: 32; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mnasnet -- Batch: 64; Input: 224
INFO:A6000:Benchmarking mnasnet -- Batch: 64; Input: 224
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[224], iters=10, model='mnasnet', model_config='pytorch/config/models/vision.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='pytorch/experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mnasnet -- Batch: 128; Input: 224
INFO:A6000:Benchmarking mnasnet -- Batch: 128; Input: 224
Loading bert model
INFO:A6000:Loading bert model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 1; Input: 128
INFO:A6000:Benchmarking bert -- Batch: 1; Input: 128
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/modeling_utils.py:2154: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert all(
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 2; Input: 128
INFO:A6000:Benchmarking bert -- Batch: 2; Input: 128
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 4; Input: 128
INFO:A6000:Benchmarking bert -- Batch: 4; Input: 128
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 8; Input: 128
INFO:A6000:Benchmarking bert -- Batch: 8; Input: 128
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 16; Input: 128
INFO:A6000:Benchmarking bert -- Batch: 16; Input: 128
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 32; Input: 128
INFO:A6000:Benchmarking bert -- Batch: 32; Input: 128
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 64; Input: 128
INFO:A6000:Benchmarking bert -- Batch: 64; Input: 128
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 128; Input: 128
INFO:A6000:Benchmarking bert -- Batch: 128; Input: 128
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading distilbert model
INFO:A6000:Loading distilbert model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 1; Input: 128
INFO:A6000:Benchmarking distilbert -- Batch: 1; Input: 128
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/modeling_utils.py:2154: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert all(
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 2; Input: 128
INFO:A6000:Benchmarking distilbert -- Batch: 2; Input: 128
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 4; Input: 128
INFO:A6000:Benchmarking distilbert -- Batch: 4; Input: 128
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 8; Input: 128
INFO:A6000:Benchmarking distilbert -- Batch: 8; Input: 128
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 16; Input: 128
INFO:A6000:Benchmarking distilbert -- Batch: 16; Input: 128
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 32; Input: 128
INFO:A6000:Benchmarking distilbert -- Batch: 32; Input: 128
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 64; Input: 128
INFO:A6000:Benchmarking distilbert -- Batch: 64; Input: 128
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 128; Input: 128
INFO:A6000:Benchmarking distilbert -- Batch: 128; Input: 128
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading funnel_transformer model
INFO:A6000:Loading funnel_transformer model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 1; Input: 128
INFO:A6000:Benchmarking funnel_transformer -- Batch: 1; Input: 128
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/funnel/modeling_funnel.py:314: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.
  num_remove = shift * len(pooled_pos)
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/funnel/modeling_funnel.py:638: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  pooling_flag = pooling_flag and block_index > 0
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/funnel/modeling_funnel.py:481: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  shift = 2 if q_head.shape[1] != context_len else 1
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/torch/nn/modules/module.py:1130: UserWarning: FALLBACK path has been taken inside: runCudaFusionGroup. This is an indication that codegen Failed for some reason.
To debug try disable codegen fallback path via setting the env variable `export PYTORCH_NVFUSER_DISABLE=fallback`
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484683044/work/torch/csrc/jit/codegen/cuda/manager.cpp:329.)
  return forward_call(*input, **kwargs)
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 2; Input: 128
INFO:A6000:Benchmarking funnel_transformer -- Batch: 2; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 4; Input: 128
INFO:A6000:Benchmarking funnel_transformer -- Batch: 4; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 8; Input: 128
INFO:A6000:Benchmarking funnel_transformer -- Batch: 8; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 16; Input: 128
INFO:A6000:Benchmarking funnel_transformer -- Batch: 16; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 32; Input: 128
INFO:A6000:Benchmarking funnel_transformer -- Batch: 32; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 64; Input: 128
INFO:A6000:Benchmarking funnel_transformer -- Batch: 64; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 128; Input: 128
INFO:A6000:Benchmarking funnel_transformer -- Batch: 128; Input: 128
Loading albert model
INFO:A6000:Loading albert model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 1; Input: 128
INFO:A6000:Benchmarking albert -- Batch: 1; Input: 128
Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/modeling_utils.py:2154: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert all(
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 2; Input: 128
INFO:A6000:Benchmarking albert -- Batch: 2; Input: 128
Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 4; Input: 128
INFO:A6000:Benchmarking albert -- Batch: 4; Input: 128
Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 8; Input: 128
INFO:A6000:Benchmarking albert -- Batch: 8; Input: 128
Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 16; Input: 128
INFO:A6000:Benchmarking albert -- Batch: 16; Input: 128
Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 32; Input: 128
INFO:A6000:Benchmarking albert -- Batch: 32; Input: 128
Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 64; Input: 128
INFO:A6000:Benchmarking albert -- Batch: 64; Input: 128
Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 128; Input: 128
INFO:A6000:Benchmarking albert -- Batch: 128; Input: 128
Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading longformer model
INFO:A6000:Loading longformer model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 1; Input: 128
INFO:A6000:Benchmarking longformer -- Batch: 1; Input: 128
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1544: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if padding_len > 0:
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1247: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  is_global_attn = is_index_global_attn.flatten().any().item()
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:580: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert (
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:801: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert (
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:804: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert query.size() == key.size()
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:806: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  chunks_count = seq_len // window_overlap - 1
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:769: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  hidden_states.size(1) // (window_overlap * 2),
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:609: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert list(attn_scores.size()) == [
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:869: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert seq_len % (window_overlap * 2) == 0
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:870: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert attn_probs.size()[:3] == value.size()[:3]
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:871: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert attn_probs.size(3) == 2 * window_overlap + 1
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:872: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  chunks_count = seq_len // window_overlap - 1
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:876: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  batch_size * num_heads, seq_len // window_overlap, window_overlap, 2 * window_overlap + 1
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:680: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert attn_output.size() == (batch_size, seq_len, self.num_heads, self.head_dim), "Unexpected size"
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/modeling_utils.py:2154: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert all(
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py:1681: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if padding_len > 0:
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 2; Input: 128
INFO:A6000:Benchmarking longformer -- Batch: 2; Input: 128
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 4; Input: 128
INFO:A6000:Benchmarking longformer -- Batch: 4; Input: 128
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 8; Input: 128
INFO:A6000:Benchmarking longformer -- Batch: 8; Input: 128
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 16; Input: 128
INFO:A6000:Benchmarking longformer -- Batch: 16; Input: 128
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 32; Input: 128
INFO:A6000:Benchmarking longformer -- Batch: 32; Input: 128
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 64; Input: 128
INFO:A6000:Benchmarking longformer -- Batch: 64; Input: 128
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 128; Input: 128
INFO:A6000:Benchmarking longformer -- Batch: 128; Input: 128
Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading mobile_bert model
INFO:A6000:Loading mobile_bert model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 1; Input: 128
INFO:A6000:Benchmarking mobile_bert -- Batch: 1; Input: 128
Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing MobileBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MobileBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/jaredfer/anaconda3/envs/device_benchmarking/lib/python3.8/site-packages/transformers/models/mobilebert/modeling_mobilebert.py:522: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
  torch.tensor(1000),
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 2; Input: 128
INFO:A6000:Benchmarking mobile_bert -- Batch: 2; Input: 128
Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing MobileBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MobileBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 4; Input: 128
INFO:A6000:Benchmarking mobile_bert -- Batch: 4; Input: 128
Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing MobileBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MobileBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 8; Input: 128
INFO:A6000:Benchmarking mobile_bert -- Batch: 8; Input: 128
Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing MobileBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MobileBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 16; Input: 128
INFO:A6000:Benchmarking mobile_bert -- Batch: 16; Input: 128
Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing MobileBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MobileBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 32; Input: 128
INFO:A6000:Benchmarking mobile_bert -- Batch: 32; Input: 128
Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing MobileBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MobileBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 64; Input: 128
INFO:A6000:Benchmarking mobile_bert -- Batch: 64; Input: 128
Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing MobileBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MobileBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 128; Input: 128
INFO:A6000:Benchmarking mobile_bert -- Batch: 128; Input: 128
Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing MobileBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MobileBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading squeeze_bert model
INFO:A6000:Loading squeeze_bert model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 1; Input: 128
INFO:A6000:Benchmarking squeeze_bert -- Batch: 1; Input: 128
Some weights of the model checkpoint at squeezebert/squeezebert-uncased were not used when initializing SqueezeBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing SqueezeBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SqueezeBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 2; Input: 128
INFO:A6000:Benchmarking squeeze_bert -- Batch: 2; Input: 128
Some weights of the model checkpoint at squeezebert/squeezebert-uncased were not used when initializing SqueezeBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing SqueezeBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SqueezeBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 4; Input: 128
INFO:A6000:Benchmarking squeeze_bert -- Batch: 4; Input: 128
Some weights of the model checkpoint at squeezebert/squeezebert-uncased were not used when initializing SqueezeBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing SqueezeBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SqueezeBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 8; Input: 128
INFO:A6000:Benchmarking squeeze_bert -- Batch: 8; Input: 128
Some weights of the model checkpoint at squeezebert/squeezebert-uncased were not used when initializing SqueezeBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing SqueezeBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SqueezeBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 16; Input: 128
INFO:A6000:Benchmarking squeeze_bert -- Batch: 16; Input: 128
Some weights of the model checkpoint at squeezebert/squeezebert-uncased were not used when initializing SqueezeBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing SqueezeBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SqueezeBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 32; Input: 128
INFO:A6000:Benchmarking squeeze_bert -- Batch: 32; Input: 128
Some weights of the model checkpoint at squeezebert/squeezebert-uncased were not used when initializing SqueezeBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing SqueezeBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SqueezeBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 64; Input: 128
INFO:A6000:Benchmarking squeeze_bert -- Batch: 64; Input: 128
Some weights of the model checkpoint at squeezebert/squeezebert-uncased were not used when initializing SqueezeBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing SqueezeBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SqueezeBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
INFO:A6000:Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 128; Input: 128
INFO:A6000:Benchmarking squeeze_bert -- Batch: 128; Input: 128
Some weights of the model checkpoint at squeezebert/squeezebert-uncased were not used when initializing SqueezeBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing SqueezeBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing SqueezeBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
