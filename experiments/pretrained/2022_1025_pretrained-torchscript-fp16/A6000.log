Loading bert model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 1; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 2; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 4; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 8; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 16; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 32; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 64; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking bert -- Batch: 128; Input: 128
Loading distilbert model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 1; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 2; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 4; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 8; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 16; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 32; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 64; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='distilbert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking distilbert -- Batch: 128; Input: 128
Loading funnel_transformer model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 1; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 2; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 4; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 8; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 16; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 32; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 64; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='funnel_transformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking funnel_transformer -- Batch: 128; Input: 128
Loading albert model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 1; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 2; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 4; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 8; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 16; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 32; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 64; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='albert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking albert -- Batch: 128; Input: 128
Loading longformer model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 1; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 2; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 4; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 8; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 16; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 32; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 64; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='longformer', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking longformer -- Batch: 128; Input: 128
Loading mobile_bert model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 1; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 2; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 4; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 8; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 16; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 32; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 64; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='mobile_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking mobile_bert -- Batch: 128; Input: 128
Loading squeeze_bert model
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 1; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 2; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 4; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 8; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 16; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 32; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 64; Input: 128
Run Parameters: Namespace(act_fn='relu', batch_size=[1, 2, 4, 8, 16, 32, 64, 128], device='cuda', device_config='pytorch/config/devices/cuda.yaml', device_idx=0, dropout=None, exp_name='pretrained-torchscript-fp16', input_size=[128], iters=10, model='squeeze_bert', model_config='pytorch/config/models/transformers.yaml', num_threads=1, platform='A6000', randomized_text=True, requires_grad=False, results_dir='experiments/pretrained', use_channels_last=False, use_cuda=True, use_dquant=False, use_fp16=True, use_ipex=False, use_jit=True, use_tensorrt=False)
Benchmarking squeeze_bert -- Batch: 128; Input: 128
