model: lstm
batch_size: [1]
seq_lens: [128] # [1, 2, 4, 8, 16, 32, 64, 128]
num_layers: [1, 2]
hidden_size: [512, 768, 1024, 2048, 4096] # [1, 2, 4, 8, 16, 32, 64, 256, 512, 1024, 2048, 3072, 4096]
bidirectional: False
act_fn: "relu"
dropout: 0.0
