model: lstm
batch_size: [1, 2, 4, 8, 16]
seq_lens: [1, 2, 4, 8, 16, 32, 64, 128]
num_layers: [1, 2, 4]
hidden_size: [512, 768, 1024, 2048, 4096]
bidirectional: False
act_fn: "relu"
dropout: 0.0
