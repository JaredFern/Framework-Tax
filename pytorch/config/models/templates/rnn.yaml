model: rnn
batch_size: [1]
seq_lens: [8] # [1, 2, 4, 8, 16, 32, 64, 128]
num_layers: [1, 2]
hidden_size: [512] # [1, 2, 4, 8, 16, 32, 64, 256, 512, 1024, 2048, 3072, 4096]
bidirectional: False
act_fn: "relu"
dropout: 0.0
